[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(reshape2)"
  },
  {
    "objectID": "EDA.html#introduction-section",
    "href": "EDA.html#introduction-section",
    "title": "EDA",
    "section": "Introduction Section",
    "text": "Introduction Section"
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "EDA",
    "section": "Data",
    "text": "Data\n\ndiabetesData &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\", show_col_types = FALSE)\n\ndiabetesData$Diabetes_binary &lt;- factor(diabetesData$Diabetes_binary,\n         levels = c(0,1),\n         labels = c(\"No diabetes\", \"Diabetes\"))\n\ndiabetesData$HighChol &lt;- factor(diabetesData$HighChol,\n         levels = c(0,1),\n         labels = c(\"No high cholesterol\", \"High cholesterol\"))\n\ndiabetesData$Smoker &lt;- factor(diabetesData$Smoker,\n         levels = c(0,1),\n         labels = c(\"Never smoked at least 100 cigs\", \"Has smoked at least 100 cigs\"))\n\ndiabetesData$PhysActivity &lt;- factor(diabetesData$PhysActivity,\n         levels = c(0,1),\n         labels = c(\"No physical activity\", \"Physical activity\"))\n\ndiabetesData$Sex &lt;- factor(diabetesData$Sex,\n         levels = c(0,1),\n         labels = c(\"Female\", \"Male\"))\n\ndiabetesData$Age &lt;- factor(diabetesData$Age,\n         levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13),\n         labels = c(\"20-24\",\"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\"51-54\",\"55-59\",\"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80 or older\"))\n\ndiabetesData$HvyAlcoholConsump &lt;- factor(diabetesData$HvyAlcoholConsump,\n         levels = c(0,1),\n         labels = c(\"No\", \"Yes\"))\n\ndiabetesData$DiffWalk &lt;- factor(diabetesData$DiffWalk,\n         levels = c(0,1),\n         labels = c(\"No serious difficulty walking up stairs\", \"Serious difficulty walking stairs\"))\n\nnewDiabetesData &lt;- diabetesData |&gt;\n  select(Diabetes_binary, HighChol, BMI, Smoker, PhysActivity, HvyAlcoholConsump, Sex, Age, DiffWalk)\n\ncolSums(is.na(newDiabetesData))\n\n  Diabetes_binary          HighChol               BMI            Smoker \n                0                 0                 0                 0 \n     PhysActivity HvyAlcoholConsump               Sex               Age \n                0                 0                 0                 0 \n         DiffWalk \n                0"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "EDA",
    "section": "Summarizations",
    "text": "Summarizations\n\nsummaryDiabetesData &lt;- newDiabetesData |&gt;\n  filter(Diabetes_binary == \"Diabetes\")\n\nmean(summaryDiabetesData$BMI)\n\n[1] 31.94401\n\ntable(summaryDiabetesData$HighChol, summaryDiabetesData$PhysActivity)\n\n                     \n                      No physical activity Physical activity\n  No high cholesterol                 4006              7654\n  High cholesterol                    9053             14633\n\ntable(summaryDiabetesData$Sex)\n\n\nFemale   Male \n 18411  16935 \n\ntable(summaryDiabetesData$DiffWalk)\n\n\nNo serious difficulty walking up stairs       Serious difficulty walking stairs \n                                  22225                                   13121 \n\n# Boxplot\nf &lt;- ggplot(newDiabetesData, aes(x = Diabetes_binary, y = BMI))\nf + geom_boxplot() +\n  labs(title = \"Boxplot of Value by Category\", x = \"Category\", y = \"Value\")\n\n\n\ncontingency_table &lt;- table(newDiabetesData$Diabetes_binary, newDiabetesData$PhysActivity)\n\nheatmap_data &lt;- as.data.frame(as.table(contingency_table))\n\n# Heatmap\ng &lt;- ggplot(heatmap_data, aes(x = Var1, y = Var2, fill = Freq))\ng + geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  labs(title = \"Heatmap of Cat1 by Cat2\", x = \"Category 1\", y = \"Category 2\")\n\n\n\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling File",
    "section": "",
    "text": "Modeling File\n\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(caret)\nlibrary(MLmetrics)\n\n\n#| echo: FALSE\ndiabetesData &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\", show_col_types = FALSE)\n\ndiabetesData$Diabetes_binary &lt;- factor(diabetesData$Diabetes_binary,\n         levels = c(0,1),\n         labels = c(\"No_diabetes\", \"Diabetes\"))\n\ndiabetesData$HighChol &lt;- factor(diabetesData$HighChol,\n         levels = c(0,1),\n         labels = c(\"No high cholesterol\", \"High cholesterol\"))\n\ndiabetesData$Smoker &lt;- factor(diabetesData$Smoker,\n         levels = c(0,1),\n         labels = c(\"Never smoked at least 100 cigs\", \"Has smoked at least 100 cigs\"))\n\ndiabetesData$PhysActivity &lt;- factor(diabetesData$PhysActivity,\n         levels = c(0,1),\n         labels = c(\"No physical activity\", \"Physical activity\"))\n\ndiabetesData$Sex &lt;- factor(diabetesData$Sex,\n         levels = c(0,1),\n         labels = c(\"Female\", \"Male\"))\n\ndiabetesData$Age &lt;- factor(diabetesData$Age,\n         levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13),\n         labels = c(\"20-24\",\"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\"51-54\",\"55-59\",\"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80 or older\"))\n\ndiabetesData$HvyAlcoholConsump &lt;- factor(diabetesData$HvyAlcoholConsump,\n         levels = c(0,1),\n         labels = c(\"No\", \"Yes\"))\n\ndiabetesData$DiffWalk &lt;- factor(diabetesData$DiffWalk,\n         levels = c(0,1),\n         labels = c(\"No serious difficulty walking up stairs\", \"Serious difficulty walking stairs\"))\n\nnewDiabetesData &lt;- diabetesData |&gt;\n  select(Diabetes_binary, HighChol, BMI, Smoker, PhysActivity, HvyAlcoholConsump, Sex, Age, DiffWalk)\n\nLog-loss helps measure how close the predicted probability is to the actual 0 or 1 value in binary classification. A high log-loss means the predicted value is further from the actual value. So we want a lower log-loss value for a better prediction model. However, we cannot compare log-loss scores from 2 different datasets to figure out which model is the best fit.\n\nset.seed(50)\n\nindex &lt;- createDataPartition(newDiabetesData$Diabetes_binary, p = 0.7, list = FALSE)\ntrainData &lt;- newDiabetesData[index, ]\ntestData &lt;- newDiabetesData[-index, ]\n\n\n# Define the control object with custom log-loss metric\ntrctrl &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 5,\n  summaryFunction = mnLogLoss,  # Use custom log-loss function\n  classProbs = TRUE,           # Needed for logistic regression\n)\n\n# Train the model\nlogFit1 &lt;- train(\n  Diabetes_binary ~ ., \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"  # Specify the metric to optimize\n)\n\n# Print the results\nprint(logFit1)\n\nGeneralized Linear Model \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142061, 142061, 142062, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3425077\n\n# Train the model\nlogFit2 &lt;- train(\n  Diabetes_binary ~ Age + Sex + HighChol + BMI + HvyAlcoholConsump, \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"  # Specify the metric to optimize\n)\n\n# Print the results\nprint(logFit2)\n\nGeneralized Linear Model \n\n177577 samples\n     5 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142062, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3501254\n\n# Train the model\nlogFit3 &lt;- train(\n  Diabetes_binary ~ Age + Sex + HighChol, \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"  # Specify the metric to optimize\n)\n\n# Print the results\nprint(logFit3)\n\nGeneralized Linear Model \n\n177577 samples\n     3 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (14), scaled (14) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142062, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3715974\n\n\n\nclassTree &lt;- train(Diabetes_binary ~ ., \n                 data = trainData, \n                 method = \"rpart\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = expand.grid(cp = seq(0,0.1, by=0.1)))\n\nWarning in train.default(x, y, weights = w, ...): The metric \"Accuracy\" was not\nin the result set. logLoss will be used instead.\n\nclassTree\n\nCART \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142061, 142062, 142061, 142062 \nResampling results across tuning parameters:\n\n  cp   logLoss  \n  0.0  0.3579417\n  0.1  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n# Define the control object with custom log-loss metric\ntrctrl2 &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 3,\n  summaryFunction = mnLogLoss,  # Use custom log-loss function\n  classProbs = TRUE,           # Needed for logistic regression\n)\n\n\nrandForest &lt;- train(Diabetes_binary ~ ., \n                 data = trainData, \n                 method = \"rf\",\n                 trControl=trctrl2,\n                 ntree = 50,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = data.frame(mtry = 1:5))\n\nWarning in train.default(x, y, weights = w, ...): The metric \"Accuracy\" was not\nin the result set. logLoss will be used instead.\n\nrandForest\n\nRandom Forest \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (3 fold, repeated 1 times) \nSummary of sample sizes: 118384, 118385, 118385 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.291537\n  2     3.738407\n  3     3.348893\n  4     3.084814\n  5     2.925879\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 5.\n\n\n\n\nComparison\n\nconfusionMatrix(data=testData$Diabetes_binary, reference = predict(logFit1, newdata = testData))\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    No_diabetes Diabetes\n  No_diabetes       64634      866\n  Diabetes           9621      982\n                                          \n               Accuracy : 0.8622          \n                 95% CI : (0.8597, 0.8646)\n    No Information Rate : 0.9757          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.1214          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.87043         \n            Specificity : 0.53139         \n         Pos Pred Value : 0.98678         \n         Neg Pred Value : 0.09262         \n             Prevalence : 0.97572         \n         Detection Rate : 0.84930         \n   Detection Prevalence : 0.86068         \n      Balanced Accuracy : 0.70091         \n                                          \n       'Positive' Class : No_diabetes     \n                                          \n\nconfusionMatrix(data=testData$Diabetes_binary, reference = predict(classTree, newdata = testData))\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    No_diabetes Diabetes\n  No_diabetes       63997     1503\n  Diabetes           9351     1252\n                                          \n               Accuracy : 0.8574          \n                 95% CI : (0.8549, 0.8599)\n    No Information Rate : 0.9638          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.1379          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8725          \n            Specificity : 0.4544          \n         Pos Pred Value : 0.9771          \n         Neg Pred Value : 0.1181          \n             Prevalence : 0.9638          \n         Detection Rate : 0.8409          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy : 0.6635          \n                                          \n       'Positive' Class : No_diabetes     \n                                          \n\nconfusionMatrix(data=testData$Diabetes_binary, reference = predict(randForest, newdata = testData))\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    No_diabetes Diabetes\n  No_diabetes       64894      606\n  Diabetes           9859      744\n                                        \n               Accuracy : 0.8625        \n                 95% CI : (0.86, 0.8649)\n    No Information Rate : 0.9823        \n    P-Value [Acc &gt; NIR] : 1             \n                                        \n                  Kappa : 0.096         \n                                        \n Mcnemar's Test P-Value : &lt;2e-16        \n                                        \n            Sensitivity : 0.86811       \n            Specificity : 0.55111       \n         Pos Pred Value : 0.99075       \n         Neg Pred Value : 0.07017       \n             Prevalence : 0.98226       \n         Detection Rate : 0.85271       \n   Detection Prevalence : 0.86068       \n      Balanced Accuracy : 0.70961       \n                                        \n       'Positive' Class : No_diabetes"
  }
]