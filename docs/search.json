[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(reshape2)"
  },
  {
    "objectID": "EDA.html#introduction-section",
    "href": "EDA.html#introduction-section",
    "title": "EDA",
    "section": "Introduction Section",
    "text": "Introduction Section\nFor this project we will look to predict whether or not a person will have diabetes based on some of their vital sign measurements and lifestyle data.\nThe Diabetes_binary response is the indicator of whether or not the person has diabetes. The HighChol variable is another binary variable indicating whether or not the patient had high cholesterol. Smoker is also binary, and someone who has not smoked at least 100 cigarettes in their life is not considered a smoker. A person’s body mass index (or BMI) is a numeric value that is also recorded; this set of patients has minimum 12 and maximum 98. A metric for each patient’s movement is also recorded, based on whether or not they have done physical activity in the past 30 days, not including their job. Alcohol consumption is considered “heavy” for adult men at 14 or more drinks per week, and for adult women at 7 or more drinks per week. This is also recorded for each patient. The DiffWalk variable is binary and helps the researcher understand if the patient may have difficulty walking or climbing stairs. Sex is taken into account for modeling diabetes likelihood. Age is split into 5-year categories, and most of the people being examined are middle-aged or older.\nExploratory Data Analysis (EDA) can allow us to have insight as to which variables might be related to the response as well as to each other. Then we can build our models accordingly, in order to most effectively predict when a person has diabetes."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "EDA",
    "section": "Data",
    "text": "Data\nFirst, I’ll read in the data and convert the binary numeric values into meaningful phrases.\n\ndiabetesData &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\", show_col_types = FALSE)\n\ndiabetesData$Diabetes_binary &lt;- factor(diabetesData$Diabetes_binary,\n         levels = c(0,1),\n         labels = c(\"No diabetes\", \"Diabetes\"))\n\ndiabetesData$HighChol &lt;- factor(diabetesData$HighChol,\n         levels = c(0,1),\n         labels = c(\"No high cholesterol\", \"High cholesterol\"))\n\ndiabetesData$Smoker &lt;- factor(diabetesData$Smoker,\n         levels = c(0,1),\n         labels = c(\"Never smoked at least 100 cigs\", \"Has smoked at least 100 cigs\"))\n\ndiabetesData$PhysActivity &lt;- factor(diabetesData$PhysActivity,\n         levels = c(0,1),\n         labels = c(\"No physical activity\", \"Physical activity\"))\n\ndiabetesData$Sex &lt;- factor(diabetesData$Sex,\n         levels = c(0,1),\n         labels = c(\"Female\", \"Male\"))\n\ndiabetesData$Age &lt;- factor(diabetesData$Age,\n         levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13),\n         labels = c(\"20-24\",\"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\"51-54\",\"55-59\",\"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80 or older\"))\n\ndiabetesData$HvyAlcoholConsump &lt;- factor(diabetesData$HvyAlcoholConsump,\n         levels = c(0,1),\n         labels = c(\"No\", \"Yes\"))\n\ndiabetesData$DiffWalk &lt;- factor(diabetesData$DiffWalk,\n         levels = c(0,1),\n         labels = c(\"No serious difficulty walking up stairs\", \"Serious difficulty walking stairs\"))\n\nnewDiabetesData &lt;- diabetesData |&gt;\n  select(Diabetes_binary, HighChol, BMI, Smoker, PhysActivity, HvyAlcoholConsump, Sex, Age, DiffWalk)\n\ncolSums(is.na(newDiabetesData))\n\n  Diabetes_binary          HighChol               BMI            Smoker \n                0                 0                 0                 0 \n     PhysActivity HvyAlcoholConsump               Sex               Age \n                0                 0                 0                 0 \n         DiffWalk \n                0 \n\n\nThere are no missing predictor values for any of the observations."
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "EDA",
    "section": "Summarizations",
    "text": "Summarizations\nFor my summarizations, since I am interested in the presence of diabetes, I will filter the dataset to only look at those that have diabetes.\n\nsummaryDiabetesData &lt;- newDiabetesData |&gt;\n  filter(Diabetes_binary == \"Diabetes\")\n\nmean(summaryDiabetesData$BMI)\n\n[1] 31.94401\n\n\nThe mean BMI for diabetic people in this dataset is about 32.\nBelow I will create contingency tables to see the counts for different levels of some of the categorical predictors\n\ntable(summaryDiabetesData$HighChol, summaryDiabetesData$PhysActivity)\n\n                     \n                      No physical activity Physical activity\n  No high cholesterol                 4006              7654\n  High cholesterol                    9053             14633\n\n\nSurprisingly, the highest count of diabetics have done physical activity in the last 30 days and have high cholesterol\n\ntable(summaryDiabetesData$Sex)\n\n\nFemale   Male \n 18411  16935 \n\n\nMost diabetics in this dataset are women, according to this table.\n\ntable(summaryDiabetesData$DiffWalk)\n\n\nNo serious difficulty walking up stairs       Serious difficulty walking stairs \n                                  22225                                   13121 \n\n\nHowever, most diabetics in this dataset do not have serious difficulty walking or climbing up stairs.\nThe boxplot below shows that, although there are lots of outliers, the majority of diabetics have a higher BMI than the majority of those without diabetes.\n\n# Boxplot\nf &lt;- ggplot(newDiabetesData, aes(x = Diabetes_binary, y = BMI))\nf + geom_boxplot() +\n  labs(title = \"Boxplot of BMI by Diabetes Category\", x = \"Category\", y = \"BMI\")\n\n\n\n\nThe heat map below demonstrates how physical activity is very strongly inversely related to diabetes.\n\ncontingency_table &lt;- table(newDiabetesData$Diabetes_binary, newDiabetesData$PhysActivity)\n\nheatmap_data &lt;- as.data.frame(as.table(contingency_table))\n\n# Heatmap\ng &lt;- ggplot(heatmap_data, aes(x = Var1, y = Var2, fill = Freq))\ng + geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  labs(title = \"Heatmap of Cat1 by Cat2\", x = \"Category 1\", y = \"Category 2\")\n\n\n\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling File",
    "section": "",
    "text": "library(tidyverse)\nlibrary(reshape2)\nlibrary(caret)\nlibrary(MLmetrics)\n\n\n#| echo: false\ndiabetesData &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\", show_col_types = FALSE)\n\ndiabetesData$Diabetes_binary &lt;- factor(diabetesData$Diabetes_binary,\n         levels = c(0,1),\n         labels = c(\"No_diabetes\", \"Diabetes\"))\n\ndiabetesData$HighChol &lt;- factor(diabetesData$HighChol,\n         levels = c(0,1),\n         labels = c(\"No high cholesterol\", \"High cholesterol\"))\n\ndiabetesData$Smoker &lt;- factor(diabetesData$Smoker,\n         levels = c(0,1),\n         labels = c(\"Never smoked at least 100 cigs\", \"Has smoked at least 100 cigs\"))\n\ndiabetesData$PhysActivity &lt;- factor(diabetesData$PhysActivity,\n         levels = c(0,1),\n         labels = c(\"No physical activity\", \"Physical activity\"))\n\ndiabetesData$Sex &lt;- factor(diabetesData$Sex,\n         levels = c(0,1),\n         labels = c(\"Female\", \"Male\"))\n\ndiabetesData$Age &lt;- factor(diabetesData$Age,\n         levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13),\n         labels = c(\"20-24\",\"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\"51-54\",\"55-59\",\"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80 or older\"))\n\ndiabetesData$HvyAlcoholConsump &lt;- factor(diabetesData$HvyAlcoholConsump,\n         levels = c(0,1),\n         labels = c(\"No\", \"Yes\"))\n\ndiabetesData$DiffWalk &lt;- factor(diabetesData$DiffWalk,\n         levels = c(0,1),\n         labels = c(\"No serious difficulty walking up stairs\", \"Serious difficulty walking stairs\"))\n\nnewDiabetesData &lt;- diabetesData |&gt;\n  select(Diabetes_binary, HighChol, BMI, Smoker, PhysActivity, HvyAlcoholConsump, Sex, Age, DiffWalk)\n\n\n\nLog loss helps measure how close the predicted probability is to the actual 0 or 1 value in binary classification. A high log-loss means the predicted value is further from the actual value. So we want a lower log-loss value for a better prediction model. However, we cannot compare log-loss scores from 2 different datasets to figure out which model is the best fit.\n\n\n\nFirst I will set the seed to allow reproducibility of these results. 70\\(\\%\\) of the data will be used to train the models, and 30\\(\\%\\) of the observations will be used to test the models’ predictions.\n\nset.seed(50)\n\nindex &lt;- createDataPartition(newDiabetesData$Diabetes_binary, p = 0.7, list = FALSE)\ntrainData &lt;- newDiabetesData[index, ]\ntestData &lt;- newDiabetesData[-index, ]\n\n\n\n\nLogistic regression is used when the response can either be a success or a failure. So we are modeling the average number of “successes”. In this case, the presence of diabetes for a person is considered a “success”, so we are able to model the probability that a person will have diabetes. Using the logistic (or logit) function, we are able to use a function of a linear model that cannot go below 0 or above 1.\nI set up a 5-fold cross-validation with log-loss as the metric we are trying to minimize.\n\ntrctrl &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 5,\n  summaryFunction = mnLogLoss,\n  classProbs = TRUE,\n)\n\nThen I trained 3 different logistic regression models, each with different combinations of the predictors.\n\n# Train the model\nlogFit1 &lt;- train(\n  Diabetes_binary ~ ., \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"\n)\n\n# Print the results\nprint(logFit1)\n\nGeneralized Linear Model \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142061, 142061, 142062, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3425077\n\n# Train the model\nlogFit2 &lt;- train(\n  Diabetes_binary ~ Age + Sex + HighChol + BMI + HvyAlcoholConsump, \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"\n)\n\n# Print the results\nprint(logFit2)\n\nGeneralized Linear Model \n\n177577 samples\n     5 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142062, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3501254\n\n# Train the model\nlogFit3 &lt;- train(\n  Diabetes_binary ~ Age + Sex + HighChol, \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"\n)\n\n# Print the results\nprint(logFit3)\n\nGeneralized Linear Model \n\n177577 samples\n     3 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (14), scaled (14) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142062, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3715974\n\n\nSince the first logistic model minimizes logLoss compared to the other two models, it is considered the best of the logistic models.\n\n\n\nThe classification tree method uses the most prevalent “class” (or level of the response) as the prediction value for each region. So in the case of the model below, the regions represent the values of each of the predictors, so there are many branches of the tree. Then the prediction values are chosen, and multiple complexity parameters are used in the classification model. The classification tree model with the lowest complexity parameter should be chosen.\n\nclassTree &lt;- train(Diabetes_binary ~ ., \n                 data = trainData, \n                 method = \"rpart\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = expand.grid(cp = seq(0,0.1, by=0.1)))\nclassTree\n\nCART \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142061, 142062, 142061, 142062 \nResampling results across tuning parameters:\n\n  cp   logLoss  \n  0.0  0.3579417\n  0.1  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\nAccording to the fit, a complexity parameter of 0 should be used in order to minimize log loss.\n\n\n\nNow 3-fold cross validation should be used for the random forest model technique. The random forest modeling averages across many different tree fits, which decreases variance over just the one classification tree fit.\n\n# Define the control object with custom log-loss metric\ntrctrl2 &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 3,\n  summaryFunction = mnLogLoss,  # Use custom log-loss function\n  classProbs = TRUE,           # Needed for logistic regression\n)\n\n\nrandForest &lt;- train(Diabetes_binary ~ ., \n                 data = trainData, \n                 method = \"rf\",\n                 trControl=trctrl2,\n                 ntree = 50,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = data.frame(mtry = 1:5))\n\nWarning in train.default(x, y, weights = w, ...): The metric \"Accuracy\" was not\nin the result set. logLoss will be used instead.\n\nrandForest\n\nRandom Forest \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (3 fold, repeated 1 times) \nSummary of sample sizes: 118384, 118385, 118385 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.291537\n  2     3.738407\n  3     3.348893\n  4     3.084814\n  5     2.925879\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 5.\n\n\nAccording to the results above, the mtry = 5 parameter minimizes the log loss for the random forest modeling technique."
  },
  {
    "objectID": "Modeling.html#log-loss",
    "href": "Modeling.html#log-loss",
    "title": "Modeling File",
    "section": "",
    "text": "Log loss helps measure how close the predicted probability is to the actual 0 or 1 value in binary classification. A high log-loss means the predicted value is further from the actual value. So we want a lower log-loss value for a better prediction model. However, we cannot compare log-loss scores from 2 different datasets to figure out which model is the best fit."
  },
  {
    "objectID": "Modeling.html#training-and-test",
    "href": "Modeling.html#training-and-test",
    "title": "Modeling File",
    "section": "",
    "text": "First I will set the seed to allow reproducibility of these results. 70\\(\\%\\) of the data will be used to train the models, and 30\\(\\%\\) of the observations will be used to test the models’ predictions.\n\nset.seed(50)\n\nindex &lt;- createDataPartition(newDiabetesData$Diabetes_binary, p = 0.7, list = FALSE)\ntrainData &lt;- newDiabetesData[index, ]\ntestData &lt;- newDiabetesData[-index, ]"
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "Modeling File",
    "section": "",
    "text": "Logistic regression is used when the response can either be a success or a failure. So we are modeling the average number of “successes”. In this case, the presence of diabetes for a person is considered a “success”, so we are able to model the probability that a person will have diabetes. Using the logistic (or logit) function, we are able to use a function of a linear model that cannot go below 0 or above 1.\nI set up a 5-fold cross-validation with log-loss as the metric we are trying to minimize.\n\ntrctrl &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 5,\n  summaryFunction = mnLogLoss,\n  classProbs = TRUE,\n)\n\nThen I trained 3 different logistic regression models, each with different combinations of the predictors.\n\n# Train the model\nlogFit1 &lt;- train(\n  Diabetes_binary ~ ., \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"\n)\n\n# Print the results\nprint(logFit1)\n\nGeneralized Linear Model \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142061, 142061, 142062, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3425077\n\n# Train the model\nlogFit2 &lt;- train(\n  Diabetes_binary ~ Age + Sex + HighChol + BMI + HvyAlcoholConsump, \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"\n)\n\n# Print the results\nprint(logFit2)\n\nGeneralized Linear Model \n\n177577 samples\n     5 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142062, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3501254\n\n# Train the model\nlogFit3 &lt;- train(\n  Diabetes_binary ~ Age + Sex + HighChol, \n  data = trainData, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trctrl,\n  preProcess = c(\"center\", \"scale\"),\n  metric = \"logLoss\"\n)\n\n# Print the results\nprint(logFit3)\n\nGeneralized Linear Model \n\n177577 samples\n     3 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (14), scaled (14) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142062, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3715974\n\n\nSince the first logistic model minimizes logLoss compared to the other two models, it is considered the best of the logistic models."
  },
  {
    "objectID": "Modeling.html#classification-tree-model",
    "href": "Modeling.html#classification-tree-model",
    "title": "Modeling File",
    "section": "",
    "text": "The classification tree method uses the most prevalent “class” (or level of the response) as the prediction value for each region. So in the case of the model below, the regions represent the values of each of the predictors, so there are many branches of the tree. Then the prediction values are chosen, and multiple complexity parameters are used in the classification model. The classification tree model with the lowest complexity parameter should be chosen.\n\nclassTree &lt;- train(Diabetes_binary ~ ., \n                 data = trainData, \n                 method = \"rpart\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = expand.grid(cp = seq(0,0.1, by=0.1)))\nclassTree\n\nCART \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (5 fold, repeated 1 times) \nSummary of sample sizes: 142062, 142061, 142062, 142061, 142062 \nResampling results across tuning parameters:\n\n  cp   logLoss  \n  0.0  0.3579417\n  0.1  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\nAccording to the fit, a complexity parameter of 0 should be used in order to minimize log loss."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling File",
    "section": "",
    "text": "Now 3-fold cross validation should be used for the random forest model technique. The random forest modeling averages across many different tree fits, which decreases variance over just the one classification tree fit.\n\n# Define the control object with custom log-loss metric\ntrctrl2 &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 3,\n  summaryFunction = mnLogLoss,  # Use custom log-loss function\n  classProbs = TRUE,           # Needed for logistic regression\n)\n\n\nrandForest &lt;- train(Diabetes_binary ~ ., \n                 data = trainData, \n                 method = \"rf\",\n                 trControl=trctrl2,\n                 ntree = 50,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = data.frame(mtry = 1:5))\n\nWarning in train.default(x, y, weights = w, ...): The metric \"Accuracy\" was not\nin the result set. logLoss will be used instead.\n\nrandForest\n\nRandom Forest \n\n177577 samples\n     8 predictor\n     2 classes: 'No_diabetes', 'Diabetes' \n\nPre-processing: centered (19), scaled (19) \nResampling: Cross-Validated (3 fold, repeated 1 times) \nSummary of sample sizes: 118384, 118385, 118385 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.291537\n  2     3.738407\n  3     3.348893\n  4     3.084814\n  5     2.925879\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 5.\n\n\nAccording to the results above, the mtry = 5 parameter minimizes the log loss for the random forest modeling technique."
  }
]